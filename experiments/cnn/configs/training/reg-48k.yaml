# for use with datasets B and C,
# which have fewer training samples
# (1500 instead of 9000)
dataloader_train:
  batch_size: 128
  shuffle: true
  num_workers: 2
dataloader_val:
  batch_size: 128
  shuffle: false
  num_workers: 2
checkpoint:
  save_top_k: 1
  monitor: 'loss'
  mode: 'min'
  dirpath: 'checkpoints'
trainer:
  max_epochs: 48000
optimizer:
  lr: 0.00002
wandb:
  project: 'awd-dev'
  group: 'cnn'
